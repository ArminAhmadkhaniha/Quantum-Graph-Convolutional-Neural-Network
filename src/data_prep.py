# -*- coding: utf-8 -*-
"""github_function_creation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HOhTFKTH131R6AGTesHVNE5QPejmvjfd
"""

import torch
import torch_geometric.transforms as T
from torch_geometric.datasets import Planetoid
from torch_geometric.utils import to_dense_adj
from torch_geometric.data import Data, InMemoryDataset
from torch.utils.data import DataLoader, TensorDataset
from sklearn.decomposition import PCA

class CustomDataset(InMemoryDataset):
    def __init__(self, data, transform=None):
        super().__init__(root='data/Cora', transform=transform)
        self.data, self.slices = self.collate([data])

    def _download(self):
        pass

    def _process(self):
        pass
    def __len__(self):
        return self.data.num_nodes

    def __getitem__(self, idx):
        # Return the features and labels for the given index
        return self.data.x[idx], self.data.y[idx]


def apply_pca(features, n_components=256):
    pca = PCA(n_components=n_components)
    return torch.tensor(pca.fit_transform(features.numpy()), dtype=torch.float)


def cora_dataset_preparation():
   # Load Cora dataset
   transform = T.NormalizeFeatures()
   dataset = Planetoid(root='data/Cora', name='Cora', transform=transform)
   data = dataset[0]

   # Select two classes for binary classification
   class_0, class_1 = 2, 4
   mask = (data.y == class_0) | (data.y == class_1)
   subset_indices = mask.nonzero(as_tuple=True)[0]

   # Create a subgraph using the subset indices
   data = data.subgraph(subset_indices)

   data.edge_index = data.edge_index[:, (data.edge_index[0, :] < data.num_nodes) & (data.edge_index[1, :] < data.num_nodes)]
   # Map original edge indices to the new subgraph indices
   mapping = {old_idx: new_idx for new_idx, old_idx in enumerate(subset_indices.tolist())}
   data.edge_index = torch.tensor([[mapping.get(i.item(), -1) for i in data.edge_index[0]],
                                 [mapping.get(i.item(), -1) for i in data.edge_index[1]]], dtype=torch.long)
   # Remove edges with invalid indices (-1)
   valid_edges = (data.edge_index[0] != -1) & (data.edge_index[1] != -1)
   data.edge_index = data.edge_index[:, valid_edges]
   data.y = (data.y == class_1).long()  # Convert to binary labels
   data.x = apply_pca(data.x)

   # Create train/test split
   train_mask = torch.rand(data.num_nodes) < 0.8
   test_mask = ~train_mask

   train_indices = train_mask.nonzero(as_tuple=True)[0]
   test_indices = test_mask.nonzero(as_tuple=True)[0]

   # Create CustomDataset instances for train and test
   train_dataset = CustomDataset(data.subgraph(train_indices))
   test_dataset = CustomDataset(data.subgraph(test_indices))

   # Now you can use DataLoader with these custom datasets:
   train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
   test_loader = DataLoader(test_dataset, batch_size=32)

   # Normalize adjacency matrix
   A = to_dense_adj(data.edge_index).squeeze(0)
   A = A + torch.eye(A.shape[0])  # Add self-loops
   D_inv_sqrt = torch.diag(1.0 / torch.sqrt(A.sum(dim=1)))
   A_norm = D_inv_sqrt @ A @ D_inv_sqrt  # Symmetric normalization
   return train_loader, test_loader, A_norm, len(test_dataset)